{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f15868",
   "metadata": {},
   "source": [
    "# Selection of PET **TAU** images to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beccaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tableone import TableOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/scratch/caroline/papers/ongoing/project00/ADNI_analysis/data'\n",
    "adni1 = pd.read_csv(os.path.join(dir, 'UCBERKELEY_TAUPVC_6MM_03Sep2024.csv'))\n",
    "adni1 = adni1.rename(columns={'PTID': 'Individual', 'SCANDATE':'tau_date'})\n",
    "img = pd.read_csv('/scratch/caroline/papers/ongoing/project00/BA_predictions_2025/data/adni/adni_nifti_20250522.csv',usecols=['GUID','Individual','Timepoint', 'Scan date', 'Alt ImageID'])\n",
    "img = img.rename(columns={'Scan date':'mri_date'})\n",
    "img['mri_date'] = pd.to_datetime(img['mri_date'])\n",
    "orig = pd.read_csv(os.path.join(dir, 'adni_original/PTDEMOG_05Jun2025.csv'), \n",
    "                   usecols=['PTID','RID', 'VISDATE', 'PTGENDER', 'PTDOB', 'PTEDUCAT'])\n",
    "orig = orig.rename(columns={'PTID':'Individual'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the first tau image\n",
    "adni1 = adni1.sort_values(by=['Individual', 'tau_date'])\n",
    "adni1 = adni1.drop_duplicates(subset=['Individual'], keep='first')\n",
    "print(adni1.Individual.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work first with the only necessary variables\n",
    "adni1_id = adni1[['Individual', 'tau_date','LONIUID']]\n",
    "#merge date based on individuals that have tau\n",
    "df_id = pd.merge(adni1_id, img, on='Individual',  how='right')\n",
    "# harmonize datetime convention to calculate date difference between scans\n",
    "df_id['tau_date'] = pd.to_datetime(df_id['tau_date'])\n",
    "# difference in months\n",
    "df_id['date_diff'] = abs((df_id['mri_date'] - df_id['tau_date']).dt.days / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only images that were taken within 3 months (MRI-TAU)\n",
    "df = df_id[(df_id['date_diff'] <= 3)].copy()\n",
    "print(df.Individual.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c701862",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUID_df = df[['Individual', 'GUID', 'mri_date','Timepoint', 'Alt ImageID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd40f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_=df.copy()\n",
    "df = df_.drop(columns=['GUID', 'Alt ImageID'])\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0245ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Individual.nunique())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91518e52",
   "metadata": {},
   "source": [
    "## Organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = pd.read_csv(os.path.join(dir, 'ADNI_SCALAR_20250617.csv'))\n",
    "#scalar = scalar.drop(columns=['PTDOBMM','PTDOBYY','PTEDUCAT','PTGENDER'])\n",
    "scalar = scalar.rename(columns={'InputIds':'GUID', 'TimePoint':'Timepoint', 'EXAMDATE':'HMSTROKE_date', 'EXAMDATE.1':'CDGLOBAL_date', 'EXAMDATE.2':'DXPARK_date', 'EXAMDATE.3':'MMSCORE_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_orig =  pd.merge(scalar, orig, on=['Individual'], how='left')\n",
    "print(scalar_orig.Individual.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cafc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "merge0 = pd.merge(scalar, df, on=['Individual', 'Timepoint'], how='left')\n",
    "merge0 = merge0.drop(columns='GUID')\n",
    "merge0 = merge0.drop_duplicates()\n",
    "print(merge0.Individual.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Propagate immutable information\n",
    "# Fill missing values within each Individual using forward and backward fill\n",
    "merge1 = merge0.sort_values(by=['Individual', 'Timepoint']).copy()\n",
    "\n",
    "merge1[['PTDOBMM', 'PTDOBYY','PTGENDER','PTEDUCAT', 'HMSTROKE', 'DXPARK']] = (\n",
    "    merge1.groupby('Individual')[['PTDOBMM', 'PTDOBYY','PTGENDER','PTEDUCAT','HMSTROKE', 'DXPARK']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")\n",
    "\n",
    "merge1[['PTDOBMM', 'PTDOBYY','PTGENDER','PTEDUCAT', 'HMSTROKE', 'DXPARK']] = (\n",
    "    merge1.groupby('Individual')[['PTDOBMM', 'PTDOBYY','PTGENDER','PTEDUCAT','HMSTROKE', 'DXPARK']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")\n",
    "\n",
    "merge1[['LONIUID', 'tau_date', 'date_diff']] = (\n",
    "    merge1.groupby(['Individual','Timepoint'])[['LONIUID','tau_date','date_diff']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge1.Individual.nunique())\n",
    "print(merge1.LONIUID.nunique())\n",
    "print(merge1.GUID.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5814379",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni2 = merge1.dropna(subset='LONIUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41757aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(dir, 'adni_original/PTDEMOG_05Jun2025.csv'), \n",
    "                   usecols=['PTID','RID', 'VISDATE', 'PTGENDER', 'PTDOB', 'PTEDUCAT'])\n",
    "orig = orig.rename(columns={'PTID':'Individual', 'PTDOB':'DOB'})\n",
    "orig = orig.drop(columns=['VISDATE','RID'])\n",
    "\n",
    "tmp = orig[orig['Individual'].isin(adni2['Individual'])]\n",
    "tmp['DOB'] = pd.to_datetime(tmp['DOB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop rows with missing month or year of birth\n",
    "print(adni2.Individual.nunique())\n",
    "print(adni2.LONIUID.nunique())\n",
    "\n",
    "#change to integer\n",
    "adni2[['PTDOBMM', 'PTDOBYY']] = adni2[['PTDOBMM', 'PTDOBYY']].astype('Int64')\n",
    "\n",
    "# put MM and YY of birth together and change to datetime format\n",
    "adni2['DOB'] = adni2['PTDOBYY'].astype(str) + adni2['PTDOBMM'].astype(str)\n",
    "adni2['DOB'] = pd.to_datetime(adni2['DOB'], format='%Y%m', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8835a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_lookup = tmp.set_index('Individual')['DOB']\n",
    "tmp = pd.merge(adni2, dob_lookup, on='Individual')\n",
    "\n",
    "tmp['DOB'] = np.where(pd.notna(tmp['DOB_y']),\n",
    "                             tmp['DOB_y'],\n",
    "                             tmp['DOB_x'])\n",
    "\n",
    "print(tmp.Individual.nunique())\n",
    "adni3 = tmp.drop(columns=['DOB_x','DOB_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni3['tau_date'] = pd.to_datetime(adni3['tau_date'])\n",
    "adni3['mri_date'] = pd.to_datetime(adni3['mri_date'])\n",
    "\n",
    "# Calculate age at FDG scan\n",
    "adni3['age_TAU'] = adni3['tau_date'] - adni3['DOB']\n",
    "adni3['age_TAU'] = adni3['age_TAU']/pd.Timedelta('365.25 days')\n",
    "adni3['age_TAU'] = adni3['age_TAU'].round(2)\n",
    "# Calculate age at MRI scan\n",
    "adni3['age_MRI'] = adni3['mri_date'] - adni3['DOB']\n",
    "adni3['age_MRI'] = adni3['age_MRI']/pd.Timedelta('365.25 days')\n",
    "adni3['age_MRI'] = adni3['age_MRI'].round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni3.Individual.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9902bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_keys = adni3[['freesurfer_6_0_0_aparc_thickness_GUID']]\n",
    "tau_keys = adni3[['LONIUID']]\n",
    "tau_fs = adni1.drop(columns=['RID', 'VISCODE', 'SITEID', 'Individual', 'tau_date', 'PROCESSDATE', 'TRACER', 'TRACER_SUVR_WARNING'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc6a2",
   "metadata": {},
   "source": [
    "## Organize MOCA\n",
    "#https://www.smchealth.org/sites/main/files/file-attachments/moca-instructions-english_2010.pdf\n",
    "#https://www.smchealth.org/sites/main/files/file-attachments/moca-instructions-english_2010.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_df = pd.read_csv(os.path.join(dir, 'adni_moca_20250612.csv'))\n",
    "moca_df = moca_df[moca_df['Phase'].notna()]\n",
    "#ADjust variables\n",
    "moca_df = moca_df.drop(columns=['IMMT1W1', 'IMMT1W2', 'IMMT1W3', 'IMMT1W4', 'IMMT1W5', 'IMMT2W1', 'IMMT2W2', 'IMMT2W3', 'IMMT2W4', 'IMMT2W5' ])\n",
    "\n",
    "# Letters = 0 or 1 error gives 1 point, else is 0\n",
    "moca_df['letters_bi'] = np.where(moca_df['LETTERS'].isin([0, 1]), 1, 0)\n",
    "moca_df = moca_df.drop(columns=['LETTERS'])\n",
    "\n",
    "#FLUENCY\n",
    "# if more than 0 < 11 < 1\n",
    "moca_df['fluency'] = (moca_df['FFLUENCY'] >= 11).astype(int)\n",
    "moca_df = moca_df.drop(columns=['FFLUENCY'])\n",
    "\n",
    "# SERIAL \n",
    "#Give no (0) points for no correct subtractions, 1 point for one correction subtraction, 2 points for two-to-three correct subtractions, and 3 points if the participant successfully makes four or five correct subtractions\n",
    "serial_cols = [col for col in moca_df.columns if col.startswith('SERIAL')]\n",
    "moca_df['serial_sum'] = moca_df[serial_cols].sum(axis=1)\n",
    "moca_df['serial_sum'] = np.where(moca_df['serial_sum'].isin([4, 5]), 3, 0)\n",
    "moca_df = moca_df.drop(columns=['SERIAL1', 'SERIAL2', 'SERIAL3', 'SERIAL4', 'SERIAL5'])\n",
    "\n",
    "#DEL\n",
    "# 1=Correct with No Cue; 2=Correct with Category Cue; 3=Correct with Mult. Choice Cue; 0=Incorrect\n",
    "# 1 point only with correct with no cue\n",
    "moca_df['delw1'] = (moca_df['DELW1'] == 1).astype(int)\n",
    "moca_df['delw2'] = (moca_df['DELW2'] == 1).astype(int)\n",
    "moca_df['delw3'] = (moca_df['DELW3'] == 1).astype(int)\n",
    "moca_df['delw4'] = (moca_df['DELW4'] == 1).astype(int)\n",
    "moca_df['delw5'] = (moca_df['DELW5'] == 1).astype(int)\n",
    "moca_df = moca_df.drop(columns=['DELW1', 'DELW2', 'DELW3', 'DELW4', 'DELW5'])\n",
    "\n",
    "moca_df['moca_score'] = moca_df.iloc[:,8:].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "moca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put moca together with the data \n",
    "moca_df_short = moca_df[['InputIds','moca_score']]\n",
    "moca_df_short = moca_df_short.rename(columns={'InputIds':'GUID'})\n",
    "adni4 = pd.merge(adni3, moca_df_short, on= ['GUID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e97c4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# put moca together with the data \n",
    "# moca_df_short = moca_df[['freesurfer_6_0_0_aparc_thickness_GUID', 'moca_score']]\n",
    "# adni4 = pd.merge(adni3, moca_df_short, on= 'freesurfer_6_0_0_aparc_thickness_GUID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323eafa5",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni4[['moca_score']] = (\n",
    "    adni4.groupby(['Individual','Timepoint'])[['moca_score']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193da8c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "moca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37bd26",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni4.Individual.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6501d91",
   "metadata": {},
   "source": [
    "## Merge ADNI with biological data\n",
    "Amyloid SUVR\n",
    "ptau181 : https://pubmed.ncbi.nlm.nih.gov/32333900/\n",
    "          https://pubmed.ncbi.nlm.nih.gov/29626426/\n",
    "          https://adni.bitbucket.io/reference/docs/UGOTPTAU181/UGOT_Lab_-_ADNI_1-Go-2_-_Method_-_Plasma_P-tau181_longitudinal.pdf\n",
    "Alpha-synuclein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c10d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "adni_bio = pd.read_csv(os.path.join(dir, 'ADNI_BIOMARKERS_SUV_TAU_20250613.csv'))\n",
    "# Be sure dates are in the right format\n",
    "adni_bio['EXAMDATE_asyn'] = pd.to_datetime(adni_bio['EXAMDATE'], errors='coerce')\n",
    "adni_bio['EXAMDATE_SUVR1.11'] = pd.to_datetime(adni_bio['EXAMDATE.1'], errors='coerce')\n",
    "adni_bio['EXAMDATE_SUVR1.08'] = pd.to_datetime(adni_bio['EXAMDATE.2'], errors='coerce')\n",
    "adni_bio['EXAMDATE_ptau181'] = pd.to_datetime(adni_bio['EXAMDATE.3'], errors='coerce')\n",
    "# Exclude the examdate in wrong format\n",
    "adni_bio = adni_bio.drop(columns=['EXAMDATE', 'EXAMDATE.1','EXAMDATE.2','EXAMDATE.3', 'registry_vdate'])\n",
    "# rename columns\n",
    "adni_bio = adni_bio.rename(columns={'Result': 'alpha_syn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44126ba",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create 'amy_status' using the first non-null value from the two columns\n",
    "adni_bio['amy_status'] = np.where(pd.notna(adni_bio['SUMMARYSUVR_WHOLECEREBNORM_1.11CUTOFF']),\n",
    "                                           adni_bio['SUMMARYSUVR_WHOLECEREBNORM_1.11CUTOFF'],\n",
    "                                           adni_bio['SUMMARYSUVR_WHOLECEREBNORM_1.08CUTOFF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd01c5c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_bio_short = adni_bio[['freesurfer_6_0_0_aparc_thickness_GUID', \n",
    "                           'amy_status', 'PLASMAPTAU181','alpha_syn']]\n",
    "\n",
    "adni5 = pd.merge(adni4, adni_bio_short, on= 'freesurfer_6_0_0_aparc_thickness_GUID', how='left')\n",
    "\n",
    "adni5[['amy_status']] = (\n",
    "    adni5.groupby(['Individual','Timepoint'])[['amy_status']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")\n",
    "\n",
    "adni5_ = adni5.dropna(subset='amy_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c565bb",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "print(adni4.Individual.nunique())\n",
    "print(adni5.Individual.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820ad21",
   "metadata": {},
   "source": [
    "# Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c38ac3",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "adni_diag = pd.read_csv(os.path.join(dir, 'ADNI_DIAGNOSIS_20250616.csv'))\n",
    "\n",
    "#change missing to na, and None to na\n",
    "adni_diag = adni_diag.replace('None', np.nan)\n",
    "adni_diag = adni_diag.replace(-4, np.nan)\n",
    "adni_diag = adni_diag.replace('-4', np.nan)\n",
    "\n",
    "#Diagnosis status\n",
    "\n",
    "# varibale 'diag_current' is based on DIAGNOSIS and DXCURREN variables that should be the same\n",
    "adni_diag['diag_current'] = np.where(pd.notna(adni_diag['DIAGNOSIS']),\n",
    "                             adni_diag['DIAGNOSIS'],\n",
    "                             adni_diag['DXCURREN'])\n",
    "\n",
    "# variable diag_change is based on diag_current and DXCHANGE\n",
    "adni_diag['diag_change'] = np.where(pd.notna(adni_diag['diag_current']),\n",
    "                             adni_diag['diag_current'],\n",
    "                             adni_diag['DXCHANGE'])\n",
    "\n",
    "adni_diag['diagnosis_CD_raw'] = adni_diag['diag_change']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5991f",
   "metadata": {},
   "source": [
    "- Dictionary\n",
    "- DXCHANGE:  Which best describes the participant's change in cognitive status from last visit to current visit:\n",
    " 1=Stable: NL to NL; 2=Stable: MCI to MCI; 3=Stable: Dementia to Dementia; 4=Conversion: NL to MCI; 5=Conversion: MCI to Dementia; 6=Conversion: NL to Dementia; 7=Reversion: MCI to NL; 8=Reversion: Dementia to MCI; 9=Reversion: Dementia to NL\n",
    "- DXCURREN:\n",
    " 1=NL;2=MCI;3=AD\n",
    "- DXCONV: Has there been a conversion or reversion to NL/MCI?\n",
    " 1=Yes - Conversion;2=Yes - Reversion; 0=No\n",
    "- DXREV:  If YES - REVERSION, choose type\n",
    " 1=MCI to Normal Control; 2=AD to MCI; 3=AD to Normal Control\n",
    "- DXNORM: Normal\n",
    " 1=Yes\n",
    "- DXMCI: \n",
    " 1=Yes\n",
    "- DXMDES: If Mild Cognitive Impairment, select any that apply:\n",
    " 1=MCI - Memory features (amnestic); 2=MCI - Non-memory features (non-amnestic)\n",
    " 1=MCI (Memory features); 2=MCI (Non-memory features)\n",
    "- DXMPTR1: If MCI - Memory features, complete the following (Petersen Criteria, see procedures manual for details): i. Subjective memory complaint\n",
    " 1=Yes; 0=No\n",
    "- DXMDUE: suspected cause of MCI\n",
    " 1=MCI due to Alzheimer's Disease; 2=MCI due to other etiology\n",
    "- DXMOTHET: If MCI due to other etiology, select box(es) to indicate reason:\n",
    " 1=Fronto-temporal Dementia; 2=Parkinson's Disease; 3=Huntington's Disease; 4=Progressive Supranuclear Palsy; 8=Corticobasal Degeneration; 9=Vascular Dementia; 10=Prion-Associated Dementia; 14=Other (specify)\n",
    "- DXDDUE:  3b. Suspected cause of dementia\n",
    " 1=Dementia due to Alzheimer's Disease; 2=Dementia due to other etiology\n",
    "- DXAD: Alzheimer's disease\n",
    " 1=Yes\n",
    "- DXAPP: If Dementia due to Alzheimer's Disease, indicate likelihood:\n",
    " 1=Probable; 2=Possible\n",
    "- DXAPOSS: If Possible AD, select box(es) to indicate reason:\n",
    " 1=Atypical clinical course or features (specify); 2=Stroke(s); 3=Depression; 4=Delirium; 5=Parkinsonism; 6=Metabolic / Toxic Disorder (specify); 7=Other (specify)\n",
    "- DXPARK:  4b. Parkinsonism symptoms present?\n",
    " 1=Yes; 0=No\n",
    "- DXODES:  If Other Dementia, select box which indicates best diagnosis:\n",
    " 1=Frontal; 3=Huntington's Disease; 5=Alcohol-related Dementia; 6=NPH; 7=Major Depression; 9=Vascular Dementia; 10=Prion-Associated Dementia; 11=HIV; 12=Primary Progressive Aphasia; 13=Posterior Cortical Dysfunction; 14=Other (specify)\n",
    "- DIAGNOSIS: 1. Which best describes the participant's current diagnosis?\n",
    " 1=CN; 2=MCI; 3=Dementia\n",
    "- BCSTROKE: 10. Did subject have a stroke? ( Diagnostic Summary - Baseline Changes)\n",
    " 1=Yes; 0=No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fd6cf",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "mytable = TableOne(adni_diag, columns=['diagnosis_CD_raw', 'diag_current', 'diag_change',\n",
    "       'DXCHANGE', 'DXCURREN', 'DXCONV', 'DXREV', 'DXNORM', 'DXMCI', 'DXMDES', 'DXMPTR1',\n",
    "       'DXMDUE', 'DXMOTHET', 'DXDDUE', 'DXAD', 'DXAPP', 'DXAPOSS', 'DXPARK',\n",
    "       'DXODES', 'DIAGNOSIS', 'BCSTROKE'])\n",
    "print(mytable.tabulate(tablefmt = \"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create new diagnosis (only 1, 2, 3) and calculate conversion time from baseline\n",
    "# 1=Stable: NL to NL; 2=Stable: MCI to MCI; 3=Stable: Dementia to Dementia; \n",
    "# 4=Conversion: NL to MCI; 5=Conversion: MCI to Dementia; 6=Conversion: NL to Dementia; \n",
    "# 7=Reversion: MCI to NL; 8=Reversion: Dementia to MCI; 9=Reversion: Dementia to NL\t\n",
    "# Define mapping dictionary\n",
    "diagnosis_mapping = {\n",
    "    4: 2,\n",
    "    5: 3,  \n",
    "    6: 3,\n",
    "    7: 1,\n",
    "    8: 2,\n",
    "    9: 1\n",
    "}\n",
    "\n",
    "# Apply mapping to the column\n",
    "#remove None and convert to numeric to map properly\n",
    "adni_diag['diagnosis_CD_raw'] = adni_diag['diagnosis_CD_raw'].replace('None', np.nan)\n",
    "adni_diag['diagnosis_CD'] = adni_diag['diagnosis_CD_raw'].astype(float).replace(diagnosis_mapping)\n",
    "\n",
    "#adni_diag['diagnosis_CD'] = adni_diag['diagnosis_CD'].replace('None', np.nan)\n",
    "#adni_diag['diagnosis_CD'] = adni_diag['diagnosis_CD'].astype(float)\n",
    "adni_diag = adni_diag.dropna(subset=['diagnosis_CD'])\n",
    "\n",
    "#change to datetime\n",
    "adni_diag['EXAMDATE'] = pd.to_datetime(adni_diag['EXAMDATE'])\n",
    "# Sort by Individual and EXAMDATE to ensure proper chronological order\n",
    "adni_diag = adni_diag.sort_values(by=['Individual', 'EXAMDATE'])\n",
    "#Calculate time between tp\n",
    "adni_diag['time_from_bl'] = adni_diag.groupby('Individual')['EXAMDATE'].transform(lambda x: (x - x.min()).dt.days / 365.25)\n",
    "\n",
    "# Compute diagnosis change\n",
    "adni_diag['diagnosis_changed'] = adni_diag.groupby('Individual')['diagnosis_CD'].transform(\n",
    "    lambda x: x != x.shift()\n",
    ")\n",
    "# First record for each Individual: force to False\n",
    "adni_diag.loc[adni_diag.groupby('Individual').head(1).index, 'diagnosis_changed'] = False\n",
    "\n",
    "# Flag Ids that has changed before two years\n",
    "adni_diag['exclude_ID'] = (\n",
    "    (adni_diag['diagnosis_changed']) & \n",
    "    (adni_diag['time_from_bl'] <= 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5da805",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_diag['cn_to_mci'] = (\n",
    "    (adni_diag['diagnosis_CD'].shift() == 1) &   # Previous was MCI\n",
    "    (adni_diag['diagnosis_CD'] == 2) &           # Current is AD\n",
    "    (adni_diag['diagnosis_changed']) &           # Must be a real change\n",
    "    (adni_diag['time_from_bl'] > 0)              # Not baseline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade84e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_diag['mci_to_ad'] = (\n",
    "    (adni_diag['diagnosis_CD'].shift() == 2) &   # Previous was MCI\n",
    "    (adni_diag['diagnosis_CD'] == 3) &           # Current is AD\n",
    "    (adni_diag['diagnosis_changed']) &           # Must be a real change\n",
    "    (adni_diag['time_from_bl'] > 0)              # Not baseline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytable = TableOne(adni_diag, columns=['diagnosis_CD', 'diagnosis_changed', 'exclude_ID', 'cn_to_mci', 'mci_to_ad', 'diagnosis_CD_raw'])\n",
    "print(mytable.tabulate(tablefmt = \"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_diag[['diagnosis_CD', 'diagnosis_CD_raw']] = (\n",
    "    adni_diag.groupby(['Individual','TimePoint'])[['diagnosis_CD', 'diagnosis_CD_raw']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed07ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_diag_short = adni_diag.drop(columns=['Project', 'Individual', 'AltId', 'TimePoint', 'InputIds','DXPARK', 'DXCHANGE',\n",
    "       'DXCURREN', 'DXCONV', 'DXREV', 'DXNORM', 'DXMCI', 'DXMDES', 'DXMPTR1',\n",
    "       'DXMDUE', 'DXMOTHET', 'DXDDUE', 'DXAD', 'DXAPP', 'DXAPOSS', 'DXODES',\n",
    "       'DIAGNOSIS', 'BCSTROKE'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b3580",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni6 = pd.merge(adni5, adni_diag_short, on= 'freesurfer_6_0_0_aparc_thickness_GUID', how='left')\n",
    "adni6_ = adni6.dropna(subset='diagnosis_CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777e17b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni6.Individual.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8f50d",
   "metadata": {},
   "source": [
    "# APOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5272dd2",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_apoe = pd.read_csv(os.path.join(dir, 'ADNI_APOE_20250616.csv'))\n",
    "\n",
    "adni_apoe['APGEN1'] = adni_apoe['APGEN1'].astype(float)\n",
    "adni_apoe['APGEN2'] = adni_apoe['APGEN2'].astype(float)\n",
    "\n",
    "adni_apoe['apoe_e4'] = np.where(\n",
    "    (adni_apoe['APGEN1'] == 4) | (adni_apoe['APGEN2'] == 4),\n",
    "    1,  # True case\n",
    "    0   # False case\n",
    ")\n",
    "\n",
    "# Count number of APOE ε4 alleles (each allele == 4)\n",
    "adni_apoe['apoe_e4_count'] = (\n",
    "    (adni_apoe['APGEN1'] == 4).astype(int) + \n",
    "    (adni_apoe['APGEN2'] == 4).astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "adni_apoe[['apoe_e4', 'apoe_e4_count']] = (\n",
    "    adni_apoe.groupby(['Individual','TimePoint'])[['apoe_e4', 'apoe_e4_count']]\n",
    "    .transform(lambda group: group.bfill().ffill())\n",
    "    .reset_index(level=0, drop=True)  # Drop extra index added by groupby\n",
    ")\n",
    "\n",
    "adni_apoe_short = adni_apoe[['freesurfer_6_0_0_aparc_thickness_GUID', 'apoe_e4', 'apoe_e4_count']]\n",
    "adni7 = pd.merge(adni6, adni_apoe_short, on= 'freesurfer_6_0_0_aparc_thickness_GUID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965e470",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni7.Individual.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e70386",
   "metadata": {},
   "source": [
    "# Merge TAU fs output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d73e67",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_tau_fs = pd.merge(tau_fs, adni7, on='LONIUID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41c36a",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_tau_fs = adni_tau_fs.drop(columns=['HMSTROKE', 'DXPARK', 'PTDOBMM', 'PTDOBYY', 'diag_current', 'diag_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabf940",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_tau_fs.Individual.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4115ff",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure dates are sorted so \"first\" means earliest\n",
    "adni_tau_fs = adni_tau_fs.sort_values(by=['Individual', 'tau_date'])\n",
    "adni_tau_fs = adni_tau_fs.dropna(subset=['freesurfer_6_0_0_aparc_thickness_GUID']) \n",
    "\n",
    "# Keep only the first tau_date per Individual\n",
    "tmp = adni_tau_fs.drop_duplicates(subset='Individual', keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f98dd6",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "tmp.Individual.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8a0bb",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b677b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "adni_tau_fs.columns[320:370]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ac24c",
   "metadata": {},
   "source": [
    "# TABLE 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fdadb",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "#table_df = adni_tau_fs.copy()\n",
    "table_df = adni7.copy()\n",
    "\n",
    "# Cleaning of data\n",
    "\n",
    "table_df['CDGLOBAL'] = table_df['CDGLOBAL'].replace(-1, np.nan)\n",
    "#table_df['PLASMAPTAU181'] = table_df['PLASMAPTAU181'].replace('None', np.nan)\n",
    "# Alpha synuclein\n",
    "# https://alz-journals.onlinelibrary.wiley.com/doi/full/10.1002/alz.14571\n",
    "# \"CSF samples were classified into one of four categories: “PD/DLB-like Detected” (Type 1) if α-syn aggregates were consistent with seeds observed in Parkinson's disease and dementia with Lewy bodies (DLB); \n",
    "# “MSA-like Detected” (Type 2) if α-syn aggregates matched seeds that are typically seen in multiple system atrophy; “Not Detected” if no α-syn aggregates were observed; or “Indeterminate” if samples did not \n",
    "# yield a definite result after two tests. For all subsequent analyses in this study, only Type 1 cases (n = 196; 34 CU and 162 CI) were considered SAA+, and only “Not Detected” cases (n = 633; 147 CU and 486 CI) \n",
    "# were considered SAA–. Both Type 2 (n = 2; 1 CU and 1 CI) and Indeterminate cases (n = 7; 1 CU and 6 CI) were excluded. All CSF α-syn SAA analyses were performed with analysts blinded to participants’ \n",
    "# demographic details, clinical profiles, and AD biomarker data.\"\n",
    "\n",
    "table_df['alpha_syn_'] = table_df['alpha_syn'].replace('Detected-1', '1')\n",
    "table_df['alpha_syn_'] = table_df['alpha_syn'].replace('Detected-2', '1')\n",
    "table_df['alpha_syn_'] = table_df['alpha_syn'].replace('Not_Detected', '0')\n",
    "table_df['alpha_syn_'] = table_df['alpha_syn'].replace('Indeterminate', '2')\n",
    "table_df['alpha_syn_'] = table_df['alpha_syn'].replace('None', '0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4d5e6",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Organize groups\n",
    "#make sure data is numeric\n",
    "table_df['diagnosis_CD'] = table_df['diagnosis_CD'].astype('float')  \n",
    "table_df['amy_status'] = table_df['amy_status'].astype('float')\n",
    "\n",
    "# Create the conditions and code for groups\n",
    "conditions = [\n",
    "    (table_df['diagnosis_CD'] == 1) & (table_df['amy_status'] == 0),  # CN, amy- 0\n",
    "    (table_df['diagnosis_CD'] == 1) & (table_df['amy_status'] == 1),  # CN, amy+ 1\n",
    "    (table_df['diagnosis_CD'] == 2) & (table_df['amy_status'] == 0),  # MCI, amy- 2\n",
    "    (table_df['diagnosis_CD'] == 2) & (table_df['amy_status'] == 1),  # MCI, amy+ 3\n",
    "    (table_df['diagnosis_CD'] == 3) & (table_df['amy_status'] == 0),  # AD, amy- 4\n",
    "    (table_df['diagnosis_CD'] == 3) & (table_df['amy_status'] == 1),  # AD, amy+ 5\n",
    "]\n",
    "\n",
    "group_codes = [0, 1, 4, 2, 5, 3]\n",
    "\n",
    "table_df['group_code'] = np.select(conditions, group_codes, default=np.nan)\n",
    "\n",
    "group_labels = {\n",
    "    0: 'CN amy-',\n",
    "    1: 'CN amy+',\n",
    "    4: 'MCI amy-',\n",
    "    2: 'MCI amy+',\n",
    "    5: 'AD amy-',\n",
    "    3: 'AD amy+'\n",
    "}\n",
    "\n",
    "table_df['group_label'] = table_df['group_code'].map(group_labels)\n",
    "\n",
    "table_df = table_df[~table_df['group_code'].isin([4, 5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a979e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "a = table_df[['Individual','amy_status','diagnosis_CD']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e5e0e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "table_df1 = table_df.dropna(subset='group_code')\n",
    "columns = ['age_MRI', 'age_TAU', 'amy_status','group_label', \n",
    "           'MMSCORE','CDGLOBAL', 'moca_score', \n",
    "           'PLASMAPTAU181', 'alpha_syn', 'time_from_bl', 'diagnosis_changed', \n",
    "           'cn_to_mci', 'mci_to_ad'\n",
    "        ]\n",
    "\n",
    "categorical = ['CDGLOBAL', 'alpha_syn', 'cn_to_mci', 'mci_to_ad', 'amy_status']\n",
    "continuous = ['age_MRI', 'age_TAU', 'MMSCORE', 'moca_score' ]\n",
    "nonnormal = []\n",
    "rename={'age_MRI': 'Age at MRI scan', 'age_TAU': 'Age at FDG scan', \n",
    "        'MMSCORE':'MMSE','CDGLOBAL':'CDR-SOB', 'moca_score':'MoCA', \n",
    "        'PLASMAPTAU181':'Plasma_pTau181', 'alpha_syn':'Aplha-synuclein'}\n",
    "groupby='group_code'\n",
    "\n",
    "#https://tableone.readthedocs.io/en/latest/index.html\n",
    "mytable = TableOne(table_df1, columns=columns, \n",
    "                    categorical=categorical, continuous=continuous, \n",
    "                    groupby=groupby, \n",
    "                    nonnormal=nonnormal, \n",
    "                    rename=rename, pval=True)\n",
    "\n",
    "print(mytable.tabulate(tablefmt=\"github\"))\n",
    "#“github”, “grid”, “fancy_grid”, “rst”, “html”, and “latex”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00455ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df1.to_csv(os.path.join(dir, 'ADNI_PET_TAU_DATA_FS_20250617.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some visual stats\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=table_df1, x='group_code', y='moca_score', hue='group_code')\n",
    "#sns.regplot(data=table_df, x='group_code', y='CDGLOBAL', scatter=False, color='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
